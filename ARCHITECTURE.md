# Architecture Documentation

Complete architecture documentation for the real-time translation application with parallel transcription, translation, and grammar correction.

---

## ðŸ—ï¸ System Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Frontend (React)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Audio Captureâ”‚  â”‚ WebSocket    â”‚  â”‚ Translation  â”‚         â”‚
â”‚  â”‚ (24kHz PCM) â”‚â†’ â”‚ Connection   â”‚â†’ â”‚ Display      â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚ WebSocket
                               â”‚ (JSON messages)
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Backend (Node.js + Express)                   â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Google Speech-to-Text Stream                 â”‚  â”‚
â”‚  â”‚  â€¢ 24kHz LINEAR16 PCM                                     â”‚  â”‚
â”‚  â”‚  â€¢ Partial results (word-by-word)                       â”‚  â”‚
â”‚  â”‚  â€¢ Auto-restart every 4 minutes                         â”‚  â”‚
â”‚  â”‚  â€¢ VAD cutoff prevention (25s restart)                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                       â”‚
â”‚                         â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Parallel Processing Pipeline                 â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
â”‚  â”‚  â”‚ Translation       â”‚      â”‚ Grammar          â”‚         â”‚  â”‚
â”‚  â”‚  â”‚ Worker            â”‚      â”‚ Worker           â”‚         â”‚  â”‚
â”‚  â”‚  â”‚ (GPT-4o-mini)     â”‚      â”‚ (GPT-4o-mini)    â”‚         â”‚  â”‚
â”‚  â”‚  â”‚                   â”‚      â”‚                  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Partial: Fast   â”‚      â”‚ â€¢ Partial: Fast  â”‚         â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Final: Complete â”‚      â”‚ â€¢ Final: Qualityâ”‚         â”‚  â”‚
â”‚  â”‚  â”‚ â€¢ Streaming: Yes  â”‚      â”‚ â€¢ Streaming: No â”‚         â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚
â”‚  â”‚            â”‚                           â”‚                   â”‚  â”‚
â”‚  â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚  â”‚
â”‚  â”‚                        â”‚                                   â”‚  â”‚
â”‚  â”‚                        â–¼                                   â”‚  â”‚
â”‚  â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚  â”‚
â”‚  â”‚              â”‚ Message Queue    â”‚                          â”‚  â”‚
â”‚  â”‚              â”‚ (Sequence IDs)   â”‚                          â”‚  â”‚
â”‚  â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚                                       â”‚
â”‚                           â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Rate Limiter                                 â”‚  â”‚
â”‚  â”‚  â€¢ 4,500 RPM limit                                        â”‚  â”‚
â”‚  â”‚  â€¢ 1.8M TPM limit                                         â”‚  â”‚
â”‚  â”‚  â€¢ Exponential backoff                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                       â”‚
â”‚                         â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              OpenAI API                                   â”‚  â”‚
â”‚  â”‚  â€¢ Chat Completions (Translation)                      â”‚  â”‚
â”‚  â”‚  â€¢ Chat Completions (Grammar)                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”„ Processing Flow

### 1. Audio Capture (Frontend)

**File:** `frontend/public/audio-stream-processor.js`

```
Microphone â†’ AudioWorklet (separate thread)
  â†’ 300ms chunks with 500ms overlap
  â†’ Int16 PCM conversion (24kHz, mono)
  â†’ Base64 encoding
  â†’ WebSocket message with metadata:
     {
       type: 'audio',
       audioData: 'base64...',
       chunkIndex: 123,
       startMs: 1000,
       endMs: 1300,
       clientTimestamp: 1234567890
     }
```

**Key Parameters:**
- **Chunk size:** 300ms
- **Overlap:** 500ms (prevents word loss at boundaries)
- **Sample rate:** 24kHz
- **Format:** LINEAR16 PCM

---

### 2. Transcription (Google Speech-to-Text)

**File:** `backend/googleSpeechStream.js`

```
Audio chunks â†’ Google Speech Streaming API
  â†’ Partial results (isPartial=true) - word-by-word
  â†’ Final results (isPartial=false) - complete sentences
  â†’ Callback: onResult(transcriptText, isPartial)
```

**Key Features:**
- **Partial results:** Enabled (`interimResults: true`)
- **Enhanced model:** `latest_long` (Chirp 3) when supported
- **Auto-restart:** Every 4 minutes (before 5-min limit)
- **VAD prevention:** Restart at 25 seconds (before aggressive VAD)
- **Jitter buffer:** 100ms batching for smooth flow

**Stream Configuration:**
```javascript
{
  encoding: 'LINEAR16',
  sampleRateHertz: 24000,
  languageCode: 'en-US', // Dynamic based on sourceLang
  enableAutomaticPunctuation: true,
  useEnhanced: true, // Conditional
  model: 'latest_long', // Conditional
  interimResults: true // CRITICAL for partials
}
```

---

### 3. Parallel Processing Pipeline

**Files:**
- `backend/soloModeHandler.js` (solo mode)
- `backend/host/adapter.js` and `core/engine/coreEngine.js` (host/listener mode)

#### 3a. Partial Results (DECOUPLED - For Speed)

```
Google Speech â†’ Partial (isPartial=true)
  â”‚
  â”œâ”€â†’ PartialTranslationWorker.translatePartial()
  â”‚     â€¢ Model: gpt-4o-mini
  â”‚     â€¢ Temperature: 0.2
  â”‚     â€¢ Max tokens: 16000
  â”‚     â€¢ Streaming: true (token-by-token)
  â”‚     â€¢ Timeout: None (cancellable)
  â”‚     â†’ Frontend receives IMMEDIATELY
  â”‚
  â””â”€â†’ GrammarWorker.correctPartial()
        â€¢ Model: gpt-4o-mini
        â€¢ Temperature: 0.1
        â€¢ Max tokens: 800
        â€¢ Min length: 8 chars
        â€¢ Timeout: 2000ms
        â†’ Frontend receives separately when ready
```

**Message Flow:**
1. Translation sent immediately with `seqId`, `isPartial: true`
2. Grammar sent separately with `updateType: 'grammar'`, `hasCorrection: true`
3. Frontend merges updates incrementally via `correctedText` field

**Benefits:**
- âœ… Translation appears instantly (200-500ms faster)
- âœ… Grammar corrections update in-place when ready
- âœ… Non-blocking: Slow grammar doesn't delay translation

#### 3b. Final Results (COUPLED - For Data Integrity)

```
Google Speech â†’ Final (isPartial=false)
  â”‚
  â””â”€â†’ Promise.allSettled([
        FinalTranslationWorker.translateFinal(),
        GrammarWorker.correctFinal()
      ])
        â€¢ Both run in parallel
        â€¢ WAIT for both to complete
        â€¢ Single message with both results
        â†’ Frontend receives complete message
        â†’ Added to history with grammar-corrected original
```

**Message Format:**
```json
{
  "type": "translation",
  "seqId": 123,
  "serverTimestamp": 1234567890,
  "isPartial": false,
  "originalText": "Hello world",
  "correctedText": "Hello, world.",
  "translatedText": "Hola, mundo.",
  "hasTranslation": true,
  "hasCorrection": true
}
```

**Benefits:**
- âœ… History entries always have complete, corrected data
- âœ… Single atomic update prevents incomplete history
- âœ… Grammar-corrected original text preserved

---

### 4. Translation Workers

**Files:** `backend/translationWorkers.js`, `backend/translationWorkersRealtime.js`

#### RealtimePartialTranslationWorker (Host Mode)

**Purpose:** Ultra low-latency translations using OpenAI's Realtime API.

**Configuration:**
- **Model:** `gpt-realtime-mini`
- **Connection Strategy:** WebSocket pool. Connections are specifically closed after *each* partial response to prevent conversational context accumulation, maintaining consistent ~150-300ms latency.
- **Concurrency:** `5` parallel workers, handling up to `30` pending requests.

#### PartialTranslationWorker (Legacy/Solo)

**Purpose:** Fast, low-latency translations for live updates

**Configuration:**
- **Model:** `gpt-4o-mini` (fast, cost-effective)
- **Temperature:** `0.2` (consistent)
- **Max tokens:** `16000` (handles long passages)
- **Streaming:** `true` (token-by-token updates)
- **Concurrency:** `5` parallel requests
- **Throttle:** `2000ms` (1 request per 2 seconds)
- **Growth threshold:** `25` chars or punctuation
- **Cache:** `200` entries, `2` minute TTL

**Features:**
- âœ… Request cancellation (smart cancellation on resets)
- âœ… Larger cache for partials (frequent repeats)
- âœ… Handles incomplete sentences gracefully
- âœ… Smart reset detection (only cancels if text shrunk >40%)

#### FinalTranslationWorker

**Purpose:** Fast translations for history entries

**Configuration:**
- **Model:** `gpt-4o-mini` (fast and cost-effective)
- **Temperature:** `0.3` (balanced)
- **Max tokens:** `16000` (full context)
- **Streaming:** `false` (complete response)
- **Cache:** `100` entries, `10` minute TTL

**Features:**
- âœ… No cancellation (always completes)
- âœ… Standard cache for finals
- âœ… Complete sentence context

---

### 5. Grammar Worker

**File:** `backend/grammarWorker.js`

**Purpose:** Real-time grammar correction for English transcripts

#### Partial Grammar Correction

**Configuration:**
- **Model:** `gpt-4o-mini`
- **Temperature:** `0.1` (very consistent)
- **Max tokens:** `800` (faster responses)
- **Min length:** `8` chars (skips trivial words)
- **Throttle:** `2000ms` (1 request per 2 seconds)
- **Growth threshold:** `20` chars or punctuation
- **Timeout:** `2000ms` (prevents blocking UI)
- **Cache:** `200` entries, `2` minute TTL

**Features:**
- âœ… Handles homophones and STT mishears
- âœ… Respects biblical/church language
- âœ… Preserves meaning (no paraphrasing)
- âœ… Fast timeout prevents UI blocking

#### Final Grammar Correction

**Configuration:**
- **Model:** `gpt-4o-mini`
- **Temperature:** `0.1`
- **Max tokens:** `2000` (full context)
- **Timeout:** `5000ms` (longer for quality)

**Features:**
- âœ… Complete context for quality
- âœ… Longer timeout for accuracy

---

### 6. Rate Limiting

**File:** `backend/openaiRateLimiter.js`

**Purpose:** Prevents hitting OpenAI API rate limits

**Configuration:**
- **RPM limit:** `4,500` requests/minute (10% safety margin)
- **TPM limit:** `1,800,000` tokens/minute (10% safety margin)
- **Max retries:** `5` attempts
- **Base delay:** `1000ms` exponential backoff
- **Max delay:** `60000ms` (60 seconds)

**Features:**
- âœ… Automatic retry with exponential backoff
- âœ… Request skipping if wait > 2 seconds
- âœ… TPM/RPM limit detection and handling
- âœ… Per-minute window tracking

---

### 7. Message Sequencing

**Files:**
- `backend/soloModeHandler.js` (backend)
- `frontend/src/components/TranslationInterface.jsx` (frontend)

**Backend:**
```javascript
const sendWithSequence = (messageData, isPartial = true) => {
  const seqId = sequenceCounter++;
  latestSeqId = Math.max(latestSeqId, seqId);
  
  const message = {
    ...messageData,
    seqId,
    serverTimestamp: Date.now(),
    isPartial
  };
  
  clientWs.send(JSON.stringify(message));
};
```

**Frontend:**
```javascript
if (message.seqId <= latestSeqIdRef.current) {
  console.log(`[TranslationInterface] Dropping stale message (seq: ${message.seqId} <= ${latestSeqIdRef.current})`);
  return; // Drop stale message
}

latestSeqIdRef.current = message.seqId;
// Process message...
```

**Benefits:**
- âœ… Prevents race conditions from network reordering
- âœ… Enables accurate latency measurement
- âœ… Improves reliability under poor network conditions

---

## ðŸŽ¯ Mode-Specific Architecture

### Solo Mode

**File:** `backend/soloModeHandler.js`

**Flow:**
1. Single WebSocket connection
2. Google Speech stream initialized on `init` message
3. Audio chunks processed â†’ transcription â†’ parallel translation/grammar
4. Results sent back to same client

**Features:**
- âœ… Ultra-fast real-time settings (0ms throttle, 1-char updates)
- âœ… RTT measurement and adaptive lookahead
- âœ… Decoupled partials, coupled finals

### Host/Listener Mode

**Files:** `backend/host/adapter.js`, `core/engine/coreEngine.js`

**Flow:**
1. Host connects â†’ `host/adapter.js` delegates session management to `CoreEngine`.
2. Audio stream â†’ transcription â†’ `CoreEngine` sequences messages via `TimelineOffsetTracker`.
3. `RTTTracker` measures network latency to adjust finalization timing via `FinalizationEngine`.
4. Translated segments are broadcast directly via `SessionStore` WebSockets.

**Features:**
- âœ… Stateful orchestration separated from transport via `CoreEngine`.
- âœ… Adaptive lookahead based on Round-Trip Time (RTT).
- âœ… Real-time translations using OpenAI's Realtime API (`gpt-realtime-mini`).
- âœ… Strict segment sequencing (`seqId`) guarantees correct display order.

---

### 8. Text-to-Speech (TTS) Pipeline

**Files:** 
- `backend/tts/TtsStreamingOrchestrator.js`
- `frontend/src/tts/TtsPlayerController.js`

**Flow:**
1. Backend `TtsStreamingOrchestrator` queues finalized segments.
2. Segments are routed to providers (e.g., ElevenLabs, Google).
3. Audio chunks are streamed to frontend WebSockets.
4. Frontend `TtsPlayerController` receives chunks and queues them.

**Queue Processing & Latency:**
- **Mode 1 (Radio Mode):** Enforces *strict sequential playback*. If segment `seqId=5` is requested but delayed, segment `seqId=6` *will not play* even if ready. This prevents out-of-order audio but can artificially inflate perceived latency if a single synthesis request stalls.
- **Deduplication:** The controller actively discards duplicate segments based on text hashing to stabilize playback.

---

## ðŸ“Š Performance Characteristics

### Latency Breakdown

1. **Audio capture:** ~50-100ms (300ms chunks)
2. **Network transmission:** ~50-200ms (WebSocket)
3. **Google Speech processing:** ~200-500ms (partial results)
4. **Translation (partial):** ~200-800ms (GPT-4o-mini, streaming)
5. **Grammar (partial):** ~100-500ms (GPT-4o-mini, non-blocking)
6. **Total (partial):** ~600-2000ms end-to-end

### Throughput

- **Audio chunks:** ~3.3 chunks/second (300ms chunks)
- **Translation requests:** ~0.5 requests/second (2s throttle)
- **Grammar requests:** ~0.5 requests/second (2s throttle)
- **Concurrent translations:** Up to 5 parallel requests

### Resource Usage

- **Memory:** ~50-100MB per active session
- **CPU:** Low impact (browser handles audio encoding)
- **Network:** ~8-12 KB per 300ms audio chunk
- **API calls:** ~1-2 calls/second per session (translation + grammar)

---

## ðŸ” Security Considerations

1. **API Keys:** Never exposed to frontend, stored server-side only
2. **WebSocket:** Validated connections, session-based authentication
3. **Rate Limiting:** Prevents API abuse and quota exhaustion
4. **Error Handling:** Graceful degradation on API failures
5. **Input Validation:** Language codes validated against `languageConfig.js`

---

## ðŸš€ Scalability

### Horizontal Scaling
- **Stateless backend:** Can run multiple instances
- **Session store:** Can be moved to Redis for multi-instance support
- **Load balancing:** WebSocket connections can be load-balanced

### Vertical Scaling
- **Concurrent sessions:** Limited by server resources and API quotas
- **Rate limits:** 4,500 RPM / 1.8M TPM per instance
- **Memory:** ~50-100MB per active session

---

**Last Updated:** January 2025  
**Status:** Production-ready architecture with parallel processing optimization

